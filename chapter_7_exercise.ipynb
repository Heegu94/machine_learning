{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Exercises for chapter 7 \n",
    "___\n",
    "\n",
    "</br>\n",
    "\n",
    "### 1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, </br> is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "-> 만약 5개의 서로 다른 모델들이 모두 정확히 같은 training data를 같고 모두 95% precision(정확도)를 같는다면, 더 좋은 precision을 내는 방법인 '***voting ensemble***'을 통해 이들을 합치는 방법이 있다. \n",
    "\n",
    "-> 그리고 '*voting ensemble*'에 서로 많이 다른 모델들 (SVM classifier, Decision Tree classifier, Logistic Regression classifier와 같은)을 사용한 경우 더 좋은 precision을 낼 수 있다.\n",
    "\n",
    "-> 또한, 각 모델들이 같은 training data를 갖는다 하더라도, '***bagging***', '***pasting ensemble***' *method*를 사용하여 서로 다른 training instance에 대해 학습한 model들의 경우에도 더 좋은 precision을 보인다.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 2. What is the difference between hard and soft voting classifiers?\n",
    "\n",
    "-> ***Hard voting classifier*** : ensemble에서 각 classifier의 투표를 카운트하고, 가장 많은 투표를 받은 class를 선택한다.\n",
    "\n",
    "-> ***Soft voting classifier*** :  각 class에 대한 평균 추정 class probability를 계산하고 가장 높은 probability로 class를 선택한다.\n",
    "    </br>- 그리고 이는 high-confidence vote에 더 weight를 주며, 이는 더 잘 실행된다.\n",
    "    </br>- 하지만 *soft voting classifier* 는 포함하는 모든 classifier들이 class probability들을 추정하는 경우에만 사용가능하다. \n",
    "    </br>- (Scikit-Learn 에서의 **SVM classifier** 에서는 ***probability=True*** 로 설정하여야 한다.)\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? </br> What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "\n",
    "-> Ensemble의 각 predictor는 다른 predictor와 독립적이므로 여러 서버에 분산시킴으로써 **bagging ensemble **의 훈련 속도를 높일 수 있다.\n",
    "    </br>- 이는 bagging ensembles 뿐만 아니라 **Random Forest 와 pasting ensembles** 에도 같은 이유로 적용 가능하다.\n",
    "    \n",
    "-> **Boosting ensembles**의 경우, 각 predictor들은 이전의 predictor를 기초로 설계되어있기 때문에 (training이 불가피하게 sequential 하다), \n",
    "    \n",
    "-> **Stacking ensembles**에 관한 경우, 모든 각각의 layer에 있는 모든 predictor들은 각각 독립적이기 때문에, multiple server를 통해 parallel로 학습할 수 있다.\n",
    "    </br>- 그러나, 2번 layer에 있는 predictor들의 경우, 이전 layer에 있는 predictor들의 훈련이 끝난 후에 훈련이 가능하다.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 4. What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "-> ***OOB (*out-of-bag*) evaluation***을 통해, **Bagging ensemble** 에 있는 각 predictor는 훈련에 사용되지 않았던 training instance 를 이용해 평가된다.\n",
    "\n",
    "-> 이는 '추가적인 validation set'이 없이도 합리적으로 unbiased evaluation을 가능하도록 한다.\n",
    "\n",
    "-> 그러므로, training을 위한 instance가 많을 수록, 훈련에 사용한 ensemble이 약간더 좋아질 수 있다.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? </br> Are Extra-Trees slower or faster than regular Random Forests?\n",
    "\n",
    "-> ***Random Forests***에서 tree를 키워나갈때, 각 node에서 **Spliting**은 오직 features의 random subset에서만 고려되어진다. \n",
    "\n",
    "-> ***Extra-Trees***에서 또한 같지만, 이 method는 일반적인 *Decision Tress*처럼 best possible threshold를 찾는 것이 아니라, 각각의 feature에 대해 random 한 threshold를 사용한다.\n",
    "</br> 이러한 ***Extra-Randomness*** 는 regularization과 같은 역할을 한다 \n",
    "</br> : 만약 Random Forest가 training data에 대해 overfit하다면, ***Extra-Trees***의 경우 더 좋은 성능을 보인다.\n",
    "\n",
    "-> 게다가 ***Extra-Randomness***는 각 feature에 대해 random한 thresholds를 사용하기 때문에 Random Forests 보다 빠르게 학습(train) 한다.\n",
    "\n",
    "-> 하지만 ***Extra-Randomness***는 predictions을 도출할때, ***Random forests***보다 더 느리지도, 더 빠르지도 않다.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    "\n",
    "-> 만약, AdaBoost ensemble이 training data에 대해 underfit하다면, estimators의 수를 늘리거나, base estimator의 regularization hyperparameters를 줄이면 된다.\n",
    "\n",
    "-> 혹은 learning-rate를 약간 올려도 된다.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "\n",
    "-> 만약 ***Gradient Boosting ensemble***이 training set에 대해 overfit하다면, 먼저 learning-rate를 줄여야 한다.\n",
    "\n",
    "-> 또한 ***Early-Stopping***을 통해 가장 적절한 predictor의 수를 찾을 수도 있다.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "### 8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set </br>(e.g., use the first 40,000 instances for training, the next 10,000 for validation, and the last 10,000 for testing). </br>Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. </br>Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. </br>Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load Mnist data, and split it into a training set, a validation set and a test set (training : 40,000 instances, validation : 10,000 instances, testing : 10,000 instances)\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    mnist.target = mnist.target.astype(np.int64)\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data, mnist.target, test_size = 10000, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 10000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "#### 2. Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. \n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "Training the ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                     oob_score=False, random_state=42, verbose=0,\n",
      "                     warm_start=False)\n",
      "Training the LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Heeguen\\Miniconda3\\envs\\keras-py36\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = LinearSVC(random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)\n",
    "\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9469, 0.9492, 0.8626, 0.9603]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "> The linear SVM is far outperformed by the other classifiers. </br>\n",
    "> However, let's keep it for now since it may improve the voting classifier's performance.\n",
    "\n",
    "#### 3. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier.\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Heeguen\\Miniconda3\\envs\\keras-py36\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=10,\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_sta...\n",
       "                                            beta_2=0.999, early_stopping=False,\n",
       "                                            epsilon=1e-08,\n",
       "                                            hidden_layer_sizes=(100,),\n",
       "                                            learning_rate='constant',\n",
       "                                            learning_rate_init=0.001,\n",
       "                                            max_iter=200, momentum=0.9,\n",
       "                                            n_iter_no_change=10,\n",
       "                                            nesterovs_momentum=True,\n",
       "                                            power_t=0.5, random_state=42,\n",
       "                                            shuffle=True, solver='adam',\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=False, warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "named_estimators = [(\"random_forest_clf\", random_forest_clf),\n",
    "                   (\"extra_trees_clf\", extra_trees_clf),\n",
    "                   (\"svm_clf\", svm_clf),\n",
    "                   (\"mlp_clf\", mlp_clf)]\n",
    "\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9602"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9469, 0.9492, 0.8626, 0.9603]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Let's remove the SVM to see if performance improves. It is possible to remove an estimator by setting it to None using set_params() like this:\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=10,\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_sta...\n",
       "                                            beta_2=0.999, early_stopping=False,\n",
       "                                            epsilon=1e-08,\n",
       "                                            hidden_layer_sizes=(100,),\n",
       "                                            learning_rate='constant',\n",
       "                                            learning_rate_init=0.001,\n",
       "                                            max_iter=200, momentum=0.9,\n",
       "                                            n_iter_no_change=10,\n",
       "                                            nesterovs_momentum=True,\n",
       "                                            power_t=0.5, random_state=42,\n",
       "                                            shuffle=True, solver='adam',\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=False, warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.set_params(svm_clf=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### This updated the list of estimators:\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('random_forest_clf',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                         n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                         warm_start=False)),\n",
       " ('extra_trees_clf',\n",
       "  ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "                       oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)),\n",
       " ('svm_clf', None),\n",
       " ('mlp_clf',\n",
       "  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "                n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "                random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
       "                validation_fraction=0.1, verbose=False, warm_start=False))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### However, it did not update the list of trained estimators:\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                        warm_start=False),\n",
       " ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "                      oob_score=False, random_state=42, verbose=0,\n",
       "                      warm_start=False),\n",
       " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "           verbose=0),\n",
       " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "               random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
       "               validation_fraction=0.1, verbose=False, warm_start=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### So we can either fit the VotingClassifier again, or just remove the SVM from the list of trained estimators:\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voting_clf.estimators_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                        warm_start=False),\n",
       " ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "                      oob_score=False, random_state=42, verbose=0,\n",
       "                      warm_start=False),\n",
       " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "               random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
       "               validation_fraction=0.1, verbose=False, warm_start=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "#### Voting Classifier에서 svm을 제거한 결과 약간 더 높은 결과를 보였다. \n",
    "\n",
    "#### 이번에는 ***Soft voting***을 적용해 볼 것이다.\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"soft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9681"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "#### That's a significant improvement, and it's much better than each of the individual classifiers.\n",
    "\n",
    "### Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9437, 0.9474, 0.9585]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions </br>: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. </br>Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! </br>Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. </br>How does it compare to the voting classifier you trained earlier?\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9-1. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Train a classifier on this new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5., 5.],\n",
       "       [8., 8., 8., 8.],\n",
       "       [2., 2., 2., 2.],\n",
       "       ...,\n",
       "       [7., 7., 7., 7.],\n",
       "       [6., 6., 6., 6.],\n",
       "       [7., 7., 7., 7.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=True, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br> \n",
    "\n",
    "#### You could fine-tune this blender or try other types of blenders (e.g., an MLPClassifier), then select the best one using cross-validation, as always.\n",
    "\n",
    "#### Exercise: Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9599"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "#### This stacking ensemble does not perform as well as the soft voting classifier we trained earlier, it's just as good as the best individual classifier.\n",
    "\n",
    "</br>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
